
QUESTION:

Consider an MxN sparse matrix with Z non-zeros and a maximum of k non-zeros per
row.
    (a) How much storage (in number of words) would a CSR representation of the 
        matrix require?
    (b) How much storage (in number of words) would a JDS representation of the 
        matrix require?


ANSWER:

    (a) If there are Z non-zeros and a max of k non-zeros per row for an MxN matrix for CSR:
        - csrColIdx requires Z words, one to keep track of each non-zero's column index
        - csrData requires Z words, one for each non-zero to hold its data
        - csrRowPtr requires M+1 words, a start and end to each row, where after the first row 
        the end of one row is the start of the next (+1 comes from the last row's end ptr)

        thus, total words = Z + Z + (M + 1) = 2Z + (M + 1)

    (b) For JDS:
        - jdsRowPerm requires M, one to keep track of the original index for each input row
        - jdsRowNNZ requires M, one to keep track of the non-zero count for each row of the input
        - jdsColStartIdx requires k, to keep track of the start index of each column into jdsColIdx
        - jdsColIdx requires Z, one to keep track of each non-zero's column index
        - jdsData requires z, one for each non-zero to hold its data

        thus, total words = Z + Z + M + M + k = 2(Z + M) + k

QUESTION:

In the JDS format, what is the advantage of sorting rows according to the number 
of non-zero they contain?


ANSWER:

So when we launch the kernel, each thread in a given block performs roughly the same
amount of work. Each thread will have about the same number of elements in its row 
it calculates. This reduces divergence that could cause performance degredation.



QUESTION:

In the JDS format, what is the advantage of storing the data contiguously along 
the jagged diagonal instead of along the row?


ANSWER:

This would increase performance as storing the data in this way will result in memory 
coalescing. The consecutive threads will read consecutive elements from DRAM instead of 
jumping around in the JDS data array.



CODE:
#include <stdio.h>
#define BLOCK_SIZE 128

__global__ void spmv_csr_kernel(unsigned int dim, unsigned int *csrRowPtr, 
    unsigned int *csrColIdx, float *csrData, float *inVector, 
    float *outVector) {
    
    int row = blockDim.x * blockIdx.x + threadIdx.x;
    if (row < dim){
        float dot = 0.0;
        int row_start = csrRowPtr[row];
        int row_end = csrRowPtr[row+1];
        //Each kernel will calculate the dot of one row
        for (int i = row_start; i < row_end; i++){
            int col_idx = csrColIdx[i];
            dot += csrData[i] * inVector[col_idx];
        }
        outVector[row] += dot;
    }
}

__global__ void spmv_jds_kernel(unsigned int dim, unsigned int *jdsRowPerm, 
    unsigned int *jdsRowNNZ, unsigned int *jdsColStartIdx, 
    unsigned int *jdsColIdx, float *jdsData, float* inVector,
    float *outVector) {
    //Threads in the same block have similar amount of work (# elements to process)
    //which reduces divergence 
    int row = blockDim.x * blockIdx.x + threadIdx.x;
    if (row < dim){ //Each kernel will calculate the dot of one row
        float dot = 0.0;
        //Iterate through the jds data for the row. 
        //JDS is condensed so we just loop for # of Non-zeros in row
        for (int i = 0; i < jdsRowNNZ[row]; i++){
            //get JDS data's value * inVector's matching column value
            int jds_idx = jdsColStartIdx[i] + row;
            int col_idx = jdsColIdx[jds_idx];
            dot += jdsData[jds_idx] * inVector[col_idx];
        }
        //put our dot into the correct output vector row.
        //jdsRowPerm holds the original rows of the permuted data
        outVector[jdsRowPerm[row]] += dot;
    }
}

void spmv_csr(unsigned int dim, unsigned int *csrRowPtr, unsigned int *csrColIdx, 
    float *csrData, float *inVector, float *outVector) {

    dim3 dimGrid(ceil(float(dim)/float(BLOCK_SIZE)), 1, 1);
    dim3 dimBlock(BLOCK_SIZE, 1, 1);

    spmv_csr_kernel<<<dimGrid,dimBlock>>>(dim, csrRowPtr, csrColIdx, 
        csrData, inVector, outVector);
    cudaDeviceSynchronize(); 
}

void spmv_jds(unsigned int dim, unsigned int *jdsRowPerm, unsigned int *jdsRowNNZ, 
    unsigned int *jdsColStartIdx, unsigned int *jdsColIdx, float *jdsData, 
    float* inVector, float *outVector) {

    dim3 dimGrid(ceil(float(dim)/float(BLOCK_SIZE)), 1, 1);
    dim3 dimBlock(BLOCK_SIZE, 1, 1);

    spmv_jds_kernel<<<dimGrid, dimBlock>>>(dim, jdsRowPerm, jdsRowNNZ, jdsColStartIdx,
        jdsColIdx, jdsData, inVector, outVector);
    cudaDeviceSynchronize(); 


}










